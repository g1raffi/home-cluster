---
# Source: tempo-distributed/charts/minio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: "minio-sa"
  namespace: "demo-otel"
---
# Source: tempo-distributed/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-tempo
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: false
---
# Source: tempo-distributed/charts/minio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-minio
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
type: Opaque
data:
  rootUser: "Z3JhZmFuYS10ZW1wbw=="
  rootPassword: "c3VwZXJzZWNyZXQ="
---
# Source: tempo-distributed/charts/minio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-minio
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
      VERSIONING=$4
      OBJECTLOCKING=$5
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          ${MC} rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
    # Create the bucket if it does not exist and set objectlocking if enabled (NOTE: versioning will be not changed if OBJECTLOCKING is set because it enables versioning to the Buckets created)
    if ! checkBucketExists $BUCKET ; then
        if [ ! -z $OBJECTLOCKING ] ; then
          if [ $OBJECTLOCKING = true ] ; then
              echo "Creating bucket with OBJECTLOCKING '$BUCKET'"
              ${MC} mb --with-lock myminio/$BUCKET
          elif [ $OBJECTLOCKING = false ] ; then
                echo "Creating bucket '$BUCKET'"
                ${MC} mb myminio/$BUCKET
          fi
      elif [ -z $OBJECTLOCKING ] ; then
            echo "Creating bucket '$BUCKET'"
            ${MC} mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."  
      fi
      fi
    
    
      # set versioning for bucket if objectlocking is disabled or not set
      if [ -z $OBJECTLOCKING ] ; then
      if [ ! -z $VERSIONING ] ; then
        if [ $VERSIONING = true ] ; then
            echo "Enabling versioning for '$BUCKET'"
            ${MC} version enable myminio/$BUCKET
        elif [ $VERSIONING = false ] ; then
            echo "Suspending versioning for '$BUCKET'"
            ${MC} version suspend myminio/$BUCKET
        fi
        fi
      else
          echo "Bucket '$BUCKET' versioning unchanged."
      fi
    
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      ${MC} policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the buckets
    createBucket tempo-traces none false  
  add-user: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # AccessKey and secretkey credentials file are added to prevent shell execution errors caused by special characters.
    # Special characters for example : ',",<,>,{,}
    MINIO_ACCESSKEY_SECRETKEY_TMP="/tmp/accessKey_and_secretKey_tmp"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkUserExists ()
    # Check if the user exists, by using the exit code of `mc admin user info`
    checkUserExists() {
      CMD=$(${MC} admin user info myminio $(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP) > /dev/null 2>&1)
      return $?
    }
    
    # createUser ($policy)
    createUser() {
      POLICY=$1
      #check accessKey_and_secretKey_tmp file
      if [[ ! -f $MINIO_ACCESSKEY_SECRETKEY_TMP ]];then
        echo "credentials file does not exist"
        return 1
      fi
      if [[ $(cat $MINIO_ACCESSKEY_SECRETKEY_TMP|wc -l) -ne 2 ]];then
        echo "credentials file is invalid"
        rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
        return 1
      fi
      USER=$(head -1 $MINIO_ACCESSKEY_SECRETKEY_TMP)
      # Create the user if it does not exist
      if ! checkUserExists ; then
        echo "Creating user '$USER'"
        cat $MINIO_ACCESSKEY_SECRETKEY_TMP | ${MC} admin user add myminio
      else
        echo "User '$USER' already exists."
      fi
      #clean up credentials files.
      rm -f $MINIO_ACCESSKEY_SECRETKEY_TMP
    
      # set policy for user
      if [ ! -z $POLICY -a $POLICY != " " ] ; then
          echo "Adding policy '$POLICY' for '$USER'"
          ${MC} admin policy set myminio $POLICY user=$USER
      else
          echo "User '$USER' has no policy attached."
      fi
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
    
    # Create the users
    echo console > $MINIO_ACCESSKEY_SECRETKEY_TMP
    echo console123 >> $MINIO_ACCESSKEY_SECRETKEY_TMP
    createUser consoleAdmin
    
  add-policy: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkPolicyExists ($policy)
    # Check if the policy exists, by using the exit code of `mc admin policy info`
    checkPolicyExists() {
      POLICY=$1
      CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
      return $?
    }
    
    # createPolicy($name, $filename)
    createPolicy () {
      NAME=$1
      FILENAME=$2
    
      # Create the name if it does not exist
      echo "Checking policy: $NAME (in /config/$FILENAME.json)"
      if ! checkPolicyExists $NAME ; then
        echo "Creating policy '$NAME'"
      else
        echo "Policy '$NAME' already exists."
      fi
      ${MC} admin policy add myminio $NAME /config/$FILENAME.json
    
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
    
    
  custom-command: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    MC_CONFIG_DIR="/tmp/minio/mc/"
    MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for MinIO service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/rootUser) ; SECRET=$(cat /config/rootPassword) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to MinIO server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="${MC} alias set myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # runCommand ($@)
    # Run custom mc command
    runCommand() {
      ${MC} "$@"
      return $?
    }
    
    # Try connecting to MinIO instance
    scheme=http
    connectToMinio $scheme
---
# Source: tempo-distributed/templates/configmap-runtime.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-tempo-runtime
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "demo-otel"
data:
  overrides.yaml: |
    
    overrides:
      {}
---
# Source: tempo-distributed/templates/configmap-tempo.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-tempo-config
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
  namespace: "demo-otel"
data:
  tempo-query.yaml: |
    backend: 127.0.0.1:3100
    
  tempo.yaml: |
    
    cache:
      caches:
      - memcached:
          consistent_hash: true
          host: 'release-name-tempo-memcached'
          service: memcached-client
          timeout: 500ms
        roles:
        - parquet-footer
        - bloom
        - frontend-search
    compactor:
      compaction:
        block_retention: 48h
        compacted_block_retention: 1h
        compaction_cycle: 30s
        compaction_window: 1h
        max_block_bytes: 107374182400
        max_compaction_objects: 6000000
        max_time_per_tenant: 5m
        retention_concurrency: 10
        v2_in_buffer_bytes: 5242880
        v2_out_buffer_bytes: 20971520
        v2_prefetch_traces_count: 1000
      ring:
        kvstore:
          store: memberlist
    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318
      ring:
        kvstore:
          store: memberlist
    ingester:
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 1
        tokens_file_path: /var/tempo/tokens.json
    memberlist:
      abort_if_cluster_join_fails: false
      bind_addr: []
      bind_port: 7946
      cluster_label: 'release-name.demo-otel'
      gossip_interval: 1s
      gossip_nodes: 2
      gossip_to_dead_nodes_time: 30s
      join_members:
      - dns+release-name-tempo-gossip-ring:7946
      leave_timeout: 5s
      left_ingesters_timeout: 5m
      max_join_backoff: 1m
      max_join_retries: 10
      min_join_backoff: 1s
      node_name: ""
      packet_dial_timeout: 5s
      packet_write_timeout: 5s
      pull_push_interval: 30s
      randomize_node_name: true
      rejoin_interval: 0s
      retransmit_factor: 2
      stream_timeout: 10s
    metrics_generator:
      metrics_ingestion_time_range_slack: 30s
      processor:
        service_graphs:
          dimensions: []
          histogram_buckets:
          - 0.1
          - 0.2
          - 0.4
          - 0.8
          - 1.6
          - 3.2
          - 6.4
          - 12.8
          max_items: 10000
          wait: 10s
          workers: 10
        span_metrics:
          dimensions: []
          histogram_buckets:
          - 0.002
          - 0.004
          - 0.008
          - 0.016
          - 0.032
          - 0.064
          - 0.128
          - 0.256
          - 0.512
          - 1.02
          - 2.05
          - 4.1
      registry:
        collection_interval: 15s
        external_labels: {}
        stale_duration: 15m
      ring:
        kvstore:
          store: memberlist
      storage:
        path: /var/tempo/wal
        remote_write:
        - name: prometheus-stack
          send_exemplars: true
          url: http://prometheus-kube-prometheus-prometheus.prometheus.svc.cluster.local:9090/api/v1/write
        remote_write_add_org_id_header: true
        remote_write_flush_deadline: 20s
        wal: null
      traces_storage:
        path: /var/tempo/traces
    multitenancy_enabled: false
    overrides:
      metrics_generator_processors:
      - service-graphs
      - span-metrics
      per_tenant_override_config: /runtime-config/overrides.yaml
    querier:
      frontend_worker:
        frontend_address: release-name-tempo-query-frontend-discovery:9095
      max_concurrent_queries: 20
      search:
        external_backend: null
        external_endpoints: []
        external_hedge_requests_at: 8s
        external_hedge_requests_up_to: 2
        prefer_self: 10
        query_timeout: 30s
      trace_by_id:
        query_timeout: 10s
    query_frontend:
      max_outstanding_per_tenant: 2000
      max_retries: 2
      metrics:
        max_duration: 3h
      search:
        concurrent_jobs: 1000
        target_bytes_per_job: 104857600
      trace_by_id:
        query_shards: 50
    server:
      grpc_server_max_recv_msg_size: 4194304
      grpc_server_max_send_msg_size: 4194304
      http_listen_port: 3100
      http_server_read_timeout: 30s
      http_server_write_timeout: 30s
      log_format: logfmt
      log_level: info
    storage:
      trace:
        backend: s3
        blocklist_poll: 5m
        local:
          path: /var/tempo/traces
        pool:
          max_workers: 400
          queue_depth: 20000
        s3:
          access_key: ${MINIO_ACCESS_KEY}
          bucket: tempo-traces
          endpoint: tempo-minio:9000
          insecure: true
          secret_key: ${MINIO_SECRET_KEY}
        wal:
          path: /var/tempo/wal
    usage_report:
      reporting_enabled: true
---
# Source: tempo-distributed/charts/minio/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: release-name-minio
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "5Gi"
---
# Source: tempo-distributed/charts/minio/templates/console-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-minio-console
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9001
      protocol: TCP
      targetPort: 9001
  selector:
    app: minio
    release: release-name
---
# Source: tempo-distributed/charts/minio/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-minio
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
    monitoring: "true"
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app: minio
    release: release-name
---
# Source: tempo-distributed/templates/compactor/service-compactor.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-compactor
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: compactor
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: 3100
      protocol: TCP
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: compactor
---
# Source: tempo-distributed/templates/distributor/service-distributor-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-distributor-discovery
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
    - name: distributor-otlp-http
      port: 4318
      protocol: TCP
      targetPort: otlp-http
    - name: grpc-distributor-otlp
      port: 4317
      protocol: TCP
      targetPort: grpc-otlp
    - name: distributor-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: grpc-otlp
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
---
# Source: tempo-distributed/templates/distributor/service-distributor.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-distributor
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  internalTrafficPolicy: Cluster
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: http-metrics
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
    - name: distributor-otlp-http
      port: 4318
      protocol: TCP
      targetPort: otlp-http
    - name: grpc-distributor-otlp
      port: 4317
      protocol: TCP
      targetPort: grpc-otlp
    - name: distributor-otlp-legacy
      port: 55680
      protocol: TCP
      targetPort: grpc-otlp
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
---
# Source: tempo-distributed/templates/gossip-ring/service-gossip-ring.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-gossip-ring
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: gossip-ring
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: gossip-ring
      port: 7946
      protocol: TCP
      targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/part-of: memberlist
---
# Source: tempo-distributed/templates/ingester/service-ingester-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-ingester-discovery
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
---
# Source: tempo-distributed/templates/ingester/service-ingester.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-ingester
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
---
# Source: tempo-distributed/templates/memcached/service-memcached.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-memcached
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: memcached
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - name: memcached-client
    port: 11211
    targetPort: 11211
  - name: http-metrics
    port: 9150
    targetPort: http-metrics
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: memcached
---
# Source: tempo-distributed/templates/metrics-generator/service-metrics-generator-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-metrics-generator-discovery
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: "grpc"
      port: 9095
      protocol: TCP
      targetPort: 9095
    - name: "http-metrics"
      port: 3100
      protocol: TCP
      targetPort: 3100
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
---
# Source: tempo-distributed/templates/metrics-generator/service-metrics-generator.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-metrics-generator
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: "grpc"
      port: 9095
      protocol: TCP
      targetPort: 9095
    - name: "http-metrics"
      port: 3100
      protocol: TCP
      targetPort: 3100
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
---
# Source: tempo-distributed/templates/querier/service-querier.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-querier
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: querier
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - name: http-metrics
      port: 3100
      protocol: TCP
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: querier
---
# Source: tempo-distributed/templates/query-frontend/service-query-frontend-discovery.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-query-frontend-discovery
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 3100
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
    - name: grpclb
      port: 9096
      protocol: TCP
      targetPort: grpc
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
---
# Source: tempo-distributed/templates/query-frontend/service-query-frontend.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-tempo-query-frontend
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http-metrics
      port: 3100
      targetPort: 3100
    - name: grpc
      port: 9095
      protocol: TCP
      targetPort: 9095
  selector:
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
---
# Source: tempo-distributed/charts/minio/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-minio
  namespace: "demo-otel"
  labels:
    app: minio
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 100%
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: minio
      release: release-name
  template:
    metadata:
      name: release-name-minio
      labels:
        app: minio
        release: release-name
      annotations:
        checksum/secrets: 0f739c24bd0f97834c3fffdd85114484875a61eb04a3f3fd7d8b37e4436415e2
        checksum/config: d8793b5da2ba623561b5e506cbb8fe5b0227516713ec33e1cb377a3ad940a1b4
    spec:
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch

      serviceAccountName: minio-sa
      containers:
        - name: minio
          image: "quay.io/minio/minio:RELEASE.2022-08-13T21-54-44Z"
          imagePullPolicy: IfNotPresent
          command:
            - "/bin/sh"
            - "-ce"
            - "/usr/bin/docker-entrypoint.sh minio server /export -S /etc/minio/certs/ --address :9000 --console-address :9001"
          volumeMounts:
            - name: minio-user
              mountPath: "/tmp/credentials"
              readOnly: true
            - name: export
              mountPath: /export            
          ports:
            - name: http
              containerPort: 9000
            - name: http-console
              containerPort: 9001
          env:
            - name: MINIO_ROOT_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-minio
                  key: rootUser
            - name: MINIO_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-minio
                  key: rootPassword
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
          resources:
            requests:
              cpu: 100m
              memory: 128Mi      
      volumes:
        - name: export
          persistentVolumeClaim:
            claimName: release-name-minio
        - name: minio-user
          secret:
            secretName: release-name-minio
---
# Source: tempo-distributed/templates/compactor/deployment-compactor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-tempo-compactor
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: compactor
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: compactor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=compactor
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: compactor
          ports:
            - containerPort: 3100
              name: http-metrics
            - containerPort: 7946
              name: http-memberlist
          envFrom:
            - secretRef:
                name: minio-secret
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-compactor-store
      terminationGracePeriodSeconds: 30
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-compactor-store
          emptyDir: {}
---
# Source: tempo-distributed/templates/distributor/deployment-distributor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-tempo-distributor
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: distributor
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=distributor
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: distributor
          ports:
            - containerPort: 7946
              name: http-memberlist
              protocol: TCP
            - containerPort: 3100
              name: http-metrics
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 4317
              name: grpc-otlp
              protocol: TCP
          envFrom:
            - secretRef:
                name: minio-secret
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-distributor-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: distributor
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: distributor
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: distributor
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-distributor-store
          emptyDir: {}
---
# Source: tempo-distributed/templates/metrics-generator/deployment-metrics-generator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-tempo-metrics-generator
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: metrics-generator
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics-generator
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=metrics-generator
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: metrics-generator
          ports:
            - name: "grpc"
              containerPort: 9095
            - name: "http-memberlist"
              containerPort: 7946
            - name: "http-metrics"
              containerPort: 3100
          envFrom:
            - secretRef:
                name: minio-secret
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: wal
      terminationGracePeriodSeconds: 300
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: metrics-generator
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: metrics-generator
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: metrics-generator
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: wal
          emptyDir:
            {}
---
# Source: tempo-distributed/templates/querier/deployment-querier.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-tempo-querier
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: querier
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=querier
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: querier
          ports:
            - containerPort: 7946
              name: http-memberlist
              protocol: TCP
            - containerPort: 3100
              name: http-metrics
          envFrom:
            - secretRef:
                name: minio-secret
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-querier-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: querier
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: querier
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: querier
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-querier-store
          emptyDir: {}
---
# Source: tempo-distributed/templates/query-frontend/deployment-query-frontend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-tempo-query-frontend
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  minReadySeconds: 10
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: query-frontend
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-frontend
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - args:
            - -target=query-frontend
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: query-frontend
          ports:
            - containerPort: 3100
              name: http-metrics
            - containerPort: 9095
              name: grpc
          envFrom:
            - secretRef:
                name: minio-secret
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: tempo-queryfrontend-store
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: query-frontend
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: query-frontend
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: query-frontend
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: tempo-queryfrontend-store
          emptyDir: {}
---
# Source: tempo-distributed/templates/ingester/statefulset-ingester.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-tempo-ingester
  namespace: demo-otel
  labels:    
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: ingester
  serviceName: ingester
  podManagementPolicy: Parallel
  updateStrategy:
    rollingUpdate:
      partition: 0
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: cfca113eb3f972fe56a846c792f6afa9b7907cf318da8262b511fc1d3aac8548
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      initContainers:
        []
      containers:
        - args:
            - -target=ingester
            - -config.file=/conf/tempo.yaml
            - -mem-ballast-size-mbs=1024
            - -config.expand-env=true
          image: docker.io/grafana/tempo:2.6.0
          imagePullPolicy: IfNotPresent
          name: ingester
          ports:
            - name: grpc
              containerPort: 9095
            - name: http-memberlist
              containerPort: 7946
            - name: http-metrics
              containerPort: 3100
          envFrom:
            - secretRef:
                name: minio-secret
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 30
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
            - mountPath: /conf
              name: config
            - mountPath: /runtime-config
              name: runtime-config
            - mountPath: /var/tempo
              name: data
      terminationGracePeriodSeconds: 300
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: ingester
        
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: ingester
                topologyKey: kubernetes.io/hostname
            - weight: 75
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: ingester
                topologyKey: topology.kubernetes.io/zone
        
      volumes:
        - name: config
          configMap:
            name: release-name-tempo-config
            items:
              - key: "tempo.yaml"
                path: "tempo.yaml"
        - name: runtime-config
          configMap:
            name: release-name-tempo-runtime
            items:
              - key: "overrides.yaml"
                path: "overrides.yaml"
        - name: data
          emptyDir: {}
---
# Source: tempo-distributed/templates/memcached/statefulset-memcached.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-tempo-memcached
  namespace: demo-otel
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: memcached
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: memcached
  serviceName: memcached
  template:
    metadata:
      labels:
        helm.sh/chart: tempo-distributed-1.22.2
        app.kubernetes.io/name: tempo
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: memcached
        app.kubernetes.io/version: "2.6.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: release-name-tempo
      securityContext:
        fsGroup: 1000
      enableServiceLinks: false
      
      containers:
        - image: docker.io/memcached:1.6.29-alpine
          imagePullPolicy: IfNotPresent
          name: memcached
          ports:
            - containerPort: 11211
              name: client
          resources:
            {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: tempo
              app.kubernetes.io/instance: release-name
              app.kubernetes.io/component: memcached
        
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: tempo
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: memcached
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: tempo
                    app.kubernetes.io/instance: release-name
                    app.kubernetes.io/component: memcached
                topologyKey: topology.kubernetes.io/zone
        
  updateStrategy:
    type: RollingUpdate
---
# Source: tempo-distributed/templates/compactor/servicemonitor-compactor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-compactor
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: compactor
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/compactor"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/distributor/servicemonitor-distributor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-distributor
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: distributor
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/distributor"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/ingester/servicemonitor-ingester.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-ingester
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: ingester
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/ingester"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/memcached/servicemonitor-memcached.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-memcached
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: memcached
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: memcached
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/memcached"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/metrics-generator/servicemonitor-metrics-generator.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-metrics-generator
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: metrics-generator
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: metrics-generator
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/metrics-generator"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/querier/servicemonitor-querier.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-querier
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: querier
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/querier"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/query-frontend/servicemonitor-jaeger-query.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-tempo-query
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: tempo-query
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: tempo-query
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/tempo-query"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/templates/query-frontend/servicemonitor-query-frontend.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: release-name-tempo-query-frontend
  namespace: "demo-otel"
  labels:
    helm.sh/chart: tempo-distributed-1.22.2
    app.kubernetes.io/name: tempo
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "2.6.0"
    app.kubernetes.io/managed-by: Helm
    home-cluster: qemu-cluster
spec:
  namespaceSelector:
    matchNames:
    - demo-otel
  selector:
    matchLabels:
      app.kubernetes.io/name: tempo
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: query-frontend
    matchExpressions:
      - key: prometheus.io/service-monitor
        operator: NotIn
        values:
          - "false"
  endpoints:
    - port: http-metrics
      relabelings:
        - action: replace
          sourceLabels: [job]
          replacement: "demo-otel/query-frontend"
          targetLabel: job
      scheme: http
---
# Source: tempo-distributed/charts/minio/templates/post-install-create-bucket-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-minio-make-bucket-job
  namespace: "demo-otel"
  labels:
    app: minio-make-bucket-job
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: release-name
    spec:
      restartPolicy: OnFailure      
      volumes:
        - name: minio-configuration
          projected:
            sources:
            - configMap:
                name: release-name-minio
            - secret:
                name: release-name-minio
      containers:
      - name: minio-mc
        image: "quay.io/minio/mc:RELEASE.2022-08-11T00-30-48Z"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "/config/initialize"]
        env:
          - name: MINIO_ENDPOINT
            value: release-name-minio
          - name: MINIO_PORT
            value: "9000"
        volumeMounts:
          - name: minio-configuration
            mountPath: /config
        resources:
          requests:
            memory: 128Mi
---
# Source: tempo-distributed/charts/minio/templates/post-install-create-user-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-minio-make-user-job
  namespace: "demo-otel"
  labels:
    app: minio-make-user-job
    chart: minio-4.0.12
    release: release-name
    heritage: Helm
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app: minio-job
        release: release-name
    spec:
      restartPolicy: OnFailure      
      volumes:
        - name: minio-configuration
          projected:
            sources:
            - configMap:
                name: release-name-minio
            - secret:
                name: release-name-minio
      containers:
      - name: minio-mc
        image: "quay.io/minio/mc:RELEASE.2022-08-11T00-30-48Z"
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "/config/add-user"]
        env:
          - name: MINIO_ENDPOINT
            value: release-name-minio
          - name: MINIO_PORT
            value: "9000"
        volumeMounts:
          - name: minio-configuration
            mountPath: /config
        resources:
          requests:
            memory: 128Mi
